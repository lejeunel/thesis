@INPROCEEDINGS{chang13,
author={J. Chang and D. Wei and J. W. Fisher},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
title={A Video Representation Using Temporal Superpixels},
year={2013},
pages={2051-2058}
}

@book{hastie09,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@article{sermanet13,
  title={Overfeat: Integrated recognition, localization and detection using convolutional networks},
  author={Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6229},
  year={2013}
}

@inproceedings{zhou04,
  title={Learning with local and global consistency},
  author={Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas N and Weston, Jason and Sch{\"o}lkopf, Bernhard},
  booktitle={Advances in neural information processing systems},
  pages={321--328},
  year={2004}
}

@inproceedings{vilarino07,
author="Vilari{\~{n}}o, Fernando
and Lacey, Gerard
and Zhou, Jiang
and Mulcahy, Hugh
and Patchett, Stephen",
title="Automatic Labeling of Colonoscopy Video for Cancer Detection",
bookTitle="Iberian Conference on Pattern Recognition and Image Analysis",
year="2007",
pages="290--297",
}

@misc{BRATSChall,
title = {{BRATS Challenge}},
howpublished = {http://braintumorsegmentation.org}
}


@misc{endochal,
  author = {MICCAI},
  year="2015",
  title  = {Endoscopic Vision Challenge, 2015},
  url    = {https://endovis.grand-challenge.org/}
}

@article{achanta12,
 author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Susstrunk, Sabine},
 title = {SLIC Superpixels Compared to State-of-the-Art Superpixel Methods},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 issue_date = {November 2012},
 volume = {34},
 number = {11},
 month = nov,
 year = {2012},
 issn = {0162-8828},
 pages = {2274--2282},
 numpages = {9},
 url = {http://dx.doi.org/10.1109/TPAMI.2012.120},
 doi = {10.1109/TPAMI.2012.120},
 acmid = {2377556},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Superpixels, segmentation, clustering, k-means},
}

@article{zhang15,
abstract = {Due to the rapid technological development of various different satellite sensors, a huge volume of high-resolution image data sets can now be acquired. How to efficiently represent and recognize the scenes from such high-resolution image data has become a critical task. In this paper, we propose an unsupervised feature learning framework for scene classification. By using the saliency detection algorithm, we extract a representative set of patches from the salient regions in the image data set. These unlabeled data patches are exploited by an unsupervised feature learning method to learn a set of feature extractors which are robust and efficient and do not need elaborately designed descriptors such as the scale-invariant-feature-transform-based algorithm. We show that the statistics generated from the learned feature extractors can characterize a complex scene very well and can produce excellent classification accuracy. In order to reduce overfitting in the feature learning step, we further employ a recently developed regularization method called 'dropout,' which has proved to be very effective in image classification. In the experiments, the proposed method was applied to two challenging high-resolution data sets: the UC Merced data set containing 21 different aerial scene categories with a submeter resolution and the Sydney data set containing seven land-use categories with a 60-cm spatial resolution. The proposed method obtained results that were equal to or even better than the previous best results with the UC Merced data set, and it also obtained the highest accuracy with the Sydney data set, demonstrating that the proposed unsupervised-feature-learning-based scene classification method provides more accurate classification results than the other latent-Dirichlet-allocation-based methods and the sparse coding method.},
author = {Zhang, Fan and Du, Bo and Zhang, Liangpei},
doi = {10.1109/TGRS.2014.2357078},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Du, Zhang - 2015 - Saliency-Guided Unsupervised Feature Learning for Scene Classification(2).pdf:pdf},
isbn = {0196-2892},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Autoencoder,saliency detection,scene classification,unsupervised feature learning},
number = {4},
pages = {2175--2184},
title = {{Saliency-guided unsupervised feature learning for scene classification}},
url = {http://ieeexplore.ieee.org/ielx7/36/6919366/06910306.pdf?tp={\&}arnumber=6910306{\&}isnumber=6919366},
volume = {53},
year = {2015}
}

@article{cheriyadat14,
abstract = {The rich data provided by high-resolution satellite imagery allow us to directly model aerial scenes by understanding their spatial and structural patterns. While pixel- and object-based classification approaches are widely used for satellite image analysis, often these approaches exploit the high-fidelity image data in a limited way. In this paper, we explore an unsupervised feature learning approach for scene classification. Dense low-level feature descriptors are extracted to characterize the local spatial patterns. These unlabeled feature measurements are exploited in a novel way to learn a set of basis functions. The low-level feature descriptors are encoded in terms of the basis functions to generate new sparse representation for the feature descriptors. We show that the statistics generated from the sparse features characterize the scene well producing excellent classification accuracy. We apply our technique to several challenging aerial scene data sets: ORNL-I data set consisting of 1-m spatial resolution satellite imagery with diverse sensor and scene characteristics representing five land-use categories, UCMERCED data set representing twenty one different aerial scene categories with sub-meter resolution, and ORNL-II data set for large-facility scene detection. Our results are highly promising and, on the UCMERCED data set we outperform the previous best results. We demonstrate that the proposed aerial scene classification method can be highly effective in developing a detection system that can be used to automatically scan large-scale high-resolution satellite imagery for detecting large facilities such as a shopping mall.},
author = {Cheriyadat, Anil M},
doi = {10.1109/TGRS.2013.2241444},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheriyadat - 2014 - Unsupervised Feature Learning for Aerial Scene Classification(2).pdf:pdf},
isbn = {0196-2892},
issn = {0196-2892},
journal = {Geoscience and Remote Sensing, IEEE Transactions on},
keywords = {Aerial data,Encoding,Histograms,Kernel,ORNL-II data set,Support vector machines,UCMERCED data set,Vectors,Visualization,aerial scene classification,aerial scene data sets,basis function,classification,codebook,dense low-level feature,dictionary,feature extraction,feature learning,geophysical image processing,geophysical techniques,high-fidelity image data,high-resolution satellite imagery,image classification,object-based classification,pixel-based classification,remote sensing,satellite image analysis,sparse coding,unsupervised feature learning},
number = {1},
pages = {439--451},
title = {{Unsupervised Feature Learning for Aerial Scene Classification}},
url = {http://ieeexplore.ieee.org/ielx7/36/6675822/06472060.pdf?tp={\&}arnumber=6472060{\&}isnumber=6675822 http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472060},
volume = {52},
year = {2014}
}
@misc{openCV,
  title={Open Source Computer Vision Library},
  author={Itseez},
  year={2015},
  howpublished = {\url{https://github.com/itseez/opencv}}
}


@Misc{scipy,
  author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title =     {{SciPy}: Open source scientific tools for {Python}},
  year =      {2001--},
  url = "http://www.scipy.org/"
}

@inproceedings{yang09,
abstract = {Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a com-plexity O(n 2 ∼ n 3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scale-up the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse cod-ing followed by multi-scale spatial max pooling, and pro-pose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a num-ber of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always signif-icantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1504.06897},
author = {Yang, Jianchao and Yu, Kai and Gong, Yihong and Beckman, Thomas Huang},
booktitle = {IEEE Compmuter Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2009.5206757},
eprint = {1504.06897},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2009 - Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {1063-6919},
pages = {1794--1801},
title = {{Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification}},
year = {2009}
}

@article{lazebnik09,
abstract = {The present paper reports about an immunocytochemical inventory of the cell types involved in the metabolic activation of the tobacco-specific nitrosamine 4-(methylnitrosamino)-1-(3-pyridyl)-1-butanone (NNK) to a DNA methylating metabolite. The formation and distribution of the methylated DNA bases O6-methylguanine (O6-meGua) and 7-methylguanine (7-MeGua) were studied in respiratory tissues, oesophagus, liver, kidneys, pancreas, small intestine, colon and prostate of rat, mouse and hamster 6 h after treatment with a single dose of 30 mg NNK/kg. The tissue- and cell-specific distribution of O6-meGua- and 7-meGua-specific nuclear staining showed the same patterns and were remarkably similar in rat, mouse and hamster in spite of the diverging spectra of NNK-induced tumours in these species. In nasal tissue, a target for NNK-induced tumourigenesis in rat and hamster, but not in mouse, adduct-specific nuclear staining was observed in all three species in sustentacular cells, Bowman glands, respiratory epithelial cells and serous glands. Both methylated DNA bases were also observed in basal cells of the olfactory epithelium of rat and (occasionally) hamster, but not in those of the mouse. In the trachea, a target for NNK-induced tumourigenesis in hamster only, substantial adduct-specific nuclear staining was found in basal epithelial and glandular cells of the hamster; in the same cells of rat and mouse only a weak nuclear staining was found. In the lung, a common target for NNK-induced tumourigenesis, the formation of O6-meGua and 7-meGua was restricted predominantly to bronchial and proximal bronchiolar epithelium. Nuclear staining in the rat was occasionally found in alveolar cells and was also observed in hepatocytes. In the three species investigated, O6-meGua- and 7-MeGua-specific nuclear staining was found in target and non-target tissues. Apparently, and in analogy with results obtained in other studies, the species-specific organotropy for tumour formation of NNK is not exclusively determined by DNA methylation. Expanding methylation data with literature data on factors considered to be involved in tumour formation, namely proliferation, toxicity and DNA repair among others, still did not lead to a satisfactory explanation for the species-specific organotropy observed. Additional factors (yet to be identified), need to be taken into account in order to explain (and predict) tumourigenic effects induced by monofunctional methylating agents.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {Lazebnik, Svetlana},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazebnik et al. - Unknown - Spatial Pyramid Matching(2).pdf:pdf},
isbn = {978-3-319-10577-2},
issn = {01433334},
journal = {Object Categorization: Computer and Human Vision Perspectives},
number = {4},
pmid = {7522985},
title = {{Spatial pyramid matching}},
url = {https://pdfs.semanticscholar.org/ba8e/0bda11af08b6037666b67cf54ae1f780822d.pdf http://hal.inria.fr/docs/00/54/86/47/PDF/pyramid{\_}chapter.pdf},
volume = {3},
year = {2009}
}

@article{simonyan15,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations},
keywords = {()},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {https://arxiv.org/pdf/1409.1556.pdf http://arxiv.org/abs/1409.1556},
year = {2015}
}

@article{long15,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2015 - Fully Convolutional Networks for Semantic Segmentation ppt.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation ppt}},
year = {2015}
}
@article{ng15,
abstract = {This paper presents the techniques employed in our team's submissions to the 2015 Emotion Recognition in the Wild contest, for the sub-challenge of Static Facial Expression Recognition in the Wild. The objective of this sub-challenge is to classify the emotions expressed by the primary human subject in static images extracted from movies. We follow a transfer learning approach for deep Convolutional Neural Network (CNN) architectures. Starting from a network pre-trained on the generic ImageNet dataset, we perform supervised fine-tuning on the network in a two-stage process, first on datasets relevant to facial expressions, followed by the contest's dataset. Experimental results show that this cascading fine-tuning approach achieves better results, compared to a single stage fine-tuning with the combined datasets. Our best submission exhibited an overall accuracy of 48.5{\%} in the validation set and 55.6{\%} in the test set, which compares favorably to the respective 35.96{\%} and 39.13{\%} of the challenge baseline.},
author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
doi = {10.1145/2818346.2830593},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng et al. - Unknown - Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning.pdf:pdf},
isbn = {9781450339124},
journal = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15},
keywords = {deep learning,emotion classification,facial expression analysis},
pages = {443--449},
title = {{Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning}},
url = {http://delivery.acm.org/10.1145/2840000/2830593/p443-ng.pdf?ip=130.92.9.58{\&}id=2830593{\&}acc=ACTIVE SERVICE{\&}key=FC66C24E42F07228.E8874AA355AB3480.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=794628962{\&}CFTOKEN=13543468{\&}{\_}{\_}acm{\_}{\_}=1502261037{\_}034a078094d1f06739665bacb69},
year = {2015}
}

@article{ILSVRC15,
  Author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  Title={{ImageNet Large Scale Visual Recognition Challenge}},
  Year={2015},
  journal={International Journal of Computer Vision (IJCV)},
  doi={10.1007/s11263-015-0816-y},
  volume={115},
  number={3},
  pages={211-252}
}

@misc{simonyan14a,
author = {Simonyan, Karen and Zisserman, Andrew},
title = {{Visual Geometry Group Home Page}},
url = {http://www.robots.ox.ac.uk/{~}vgg/research/very{\_}deep/},
urldate = {2017-08-09},
year = {2014}
}

@phdthesis{imdescrip,
 title={An Unsupervised Approach to Modelling Visual Data},
 author={Steinberg, D. M.},
 year={2013}
}

@article{pan2010,
author={S. J. Pan and Q. Yang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Survey on Transfer Learning},
year={2010},
volume={22},
number={10},
pages={1345-1359},
keywords={knowledge engineering;learning by example;optimisation;unsupervised learning;data mining;inductive transfer learning;knowledge transfer;machine learning;transductive transfer learning;unsupervised transfer learning;Data mining;Knowledge engineering;Knowledge transfer;Labeling;Learning systems;Machine learning;Machine learning algorithms;Space technology;Testing;Training data;Transfer learning;data mining.;machine learning;survey},
doi={10.1109/TKDE.2009.191},
ISSN={1041-4347},
month={Oct},}

@article{vorontsov17,
abstract = {We propose a model for the joint segmentation of the liver and liver lesions in computed tomography (CT) volumes. We build the model from two fully convolutional networks con-nected in tandem and trained together end-to-end. The first network is trained to produce a representation that is used for liver segmentation. This representation is passed to ev-ery layer in the second network, the output of which is used to produce a lesion segmentation. We evaluate the approach on the 2017 ISBI Liver Tumour Segmentation Challenge and place second with a per-volume average Dice score of 0.65.},
author = {Vorontsov, Eugene and Chartrand, Gabriel and Tang, An and Pal, Chris and Kadoury, Samuel},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vorontsov et al. - Unknown - LIVER LESION SEGMENTATION INFORMED BY JOINT LIVER SEGMENTATION(2).pdf:pdf},
journal = {arXiv preprint arXiv:1707.07734},
keywords = {Index Terms— segmentation,neural network},
title = {{Liver lesion segmentation informed by joint liver segmentation}},
url = {https://arxiv.org/pdf/1707.07734.pdf https://arxiv.org/pdf/1707.07734v1.pdf},
year = {2017}
}

@article{gastaldi17,
   author = {{Gastaldi}, X.},
    title = "{Shake-Shake regularization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1705.07485},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
     year = 2017,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170507485G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{kurmann17,
author = {Kurmann, Thomas and Neila, Pablo Marquez and Du, Xiaofei and Fua, Pascal and Wolf, Sebastian and Sznitman, Raphael},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurmann et al. - 2017 - Simultaneous Recognition and Pose Estimation of Instruments in Minimally Invasive Surgery.pdf:pdf},
journal = {MICCAI 2017, Part II, LNCS 10434 proceedings},
title = {{Simultaneous Recognition and Pose Estimation of Instruments in Minimally Invasive Surgery}},
year = {2017}
}

@inproceedings{lea16,
abstract = {Joint segmentation and classification of fine-grained actions is important for applications of human-robot interaction, video surveil-lance, and human skill evaluation. However, despite substantial recent progress in large-scale action classification, the performance of state-of-the-art fine-grained action recognition approaches remains low. We propose a model for action segmentation which combines low-level spa-tiotemporal features with a high-level segmental classifier. Our spatiotem-poral CNN is comprised of a spatial component that uses convolutional filters to capture information about objects and their relationships, and a temporal component that uses large 1D convolutional filters to capture information about how object relationships change across time. These features are used in tandem with a semi-Markov model that models tran-sitions from one action to another. We introduce an efficient constrained segmental inference algorithm for this model that is orders of magnitude faster than the current approach. We highlight the effectiveness of our Segmental Spatiotemporal CNN on cooking and surgical action datasets for which we observe substantially improved performance relative to re-cent baseline methods.},
author = {Lea, Colin and Reiter, Austin and Vidal, Ren{\'{e}} and Hager, Gregory D},
booktitle = {European Conference on Computer Vision},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lea et al. - Unknown - Segmental Spatiotemporal CNNs for Fine-grained Action Segmentation.pdf:pdf},
pages = {36--52},
title = {{Segmental Spatiotemporal CNNs for Fine-grained Action Segmentation}},
url = {https://arxiv.org/pdf/1602.02995.pdf},
year = {2016}
}

@inproceedings{shi15,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the ma-chine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting prob-lem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-Chun},
booktitle = {Advances in neural information processing systems},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - Unknown - Convolutional LSTM Network A Machine Learning Approach for Precipitation Nowcasting.pdf:pdf},
pages = {802--810},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {https://arxiv.org/pdf/1506.04214.pdf},
year = {2015}
}

@article{segal04,
abstract = {Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called random forests. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.},
author = {Segal, Mark R},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Segal - 2003 - Machine Learning Benchmarks and Random Forest Regression(2).pdf:pdf},
journal = {Biostatistics},
keywords = {prediction error,regression,uci repository},
number = {MAY 2003},
pages = {1--14},
title = {{Machine Learning Benchmarks and Random Forest Regression}},
url = {http://www.biostat.ucsf.edu/cbmb/publications/bench.rf.regn.pdf http://escholarship.org/uc/item/35x3v9t4.pdf},
year = {2004}
}

@article{davis06,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - Unknown - The Relationship Between Precision-Recall and ROC Curves(2).pdf:pdf},
isbn = {1595933832},
issn = {14710080},
journal = {Proceedings of the 23rd International Conference on Machine learning -- ICML'06},
pages = {233--240},
pmid = {19165215},
title = {{The Relationship Between Precision-Recall and ROC Curves}},
url = {http://delivery.acm.org/10.1145/1150000/1143874/p233-davis.pdf?ip=130.92.9.58{\&}id=1143874{\&}acc=ACTIVE SERVICE{\&}key=FC66C24E42F07228.E8874AA355AB3480.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=797614004{\&}CFTOKEN=26964052{\&}{\_}{\_}acm{\_}{\_}=1502776330{\_}10afdf8c3302f92aa92cf52d},
year = {2006}
}

@inproceedings{csillik16,
           month = {September},
           title = {Superpixels: the end of pixels in OBIA. A comparison of stat-of-the-art superpixel methods for remote sensing data},
          author = {O. {Csillik}},
            year = {2016},
             url = {http://proceedings.utwente.nl/439/},
        abstract = {In computer vision, using superpixels or perceptually meaningful atomic regions to speed up later-stage processing are becoming increasingly popular in many applications. Superpixels are used as a pre-processing stage to organize an image into a low-level grouping process through oversegmentation, thus simplifying the computation in later stages. However, in remote sensing domain few studies use superpixels. Even so, there is no comparison between superpixel methods and their suitability for remote sensing images. In this study, we compare four state-of-the-art superpixel methods: Simple Linear Iterative Clustering (SLIC and SLICO), Superpixels Extracted via Energy-Driven Sampling (SEEDS) and Linear Spectral Clustering (LSC). We applied them to very high resolution remote sensing data of different characteristics (extent, spatial resolution and landscape complexity) in order to see how superpixels are affected by these factors. The four algorithms were compared regarding their computational time, ability to adhere to image boundaries and the accuracy of the resulted superpixels. Furthermore, we discuss the individual strengths and weaknesses of each algorithm and draw further applications of superpixels in OBIA.}
}

@inproceedings{kingma15,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
author = {Kingma, Diederik P and Adam, Jimmy Ba},
booktitle = {International Conference on Learning Representation},
doi = {10.1109/ICCCBDA.2017.7951902},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION(2).pdf:pdf},
isbn = {9781509044986},
title = {{A method for stochastic optimization}},
url = {https://arxiv.org/pdf/1412.6980.pdf},
year = {2015}
}

@article{glorot10,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}

@article{huynh16,
  title={Digital mammographic tumor classification using transfer learning from deep convolutional neural networks},
  author={Huynh, Benjamin Q and Li, Hui and Giger, Maryellen L},
  journal={Journal of Medical Imaging},
  volume={3},
  number={3},
  pages={034501},
  year={2016},
  publisher={International Society for Optics and Photonics}
}

@book{goodfellow16,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{mcculloch43,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{lecun95,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@article{lejeune18,
  title={Iterative multi-path tracking for video and volume segmentation with sparse point supervision},
  author={Lejeune, Laurent and Grossrieder, Jan and Sznitman, Raphael},
  journal={Medical image analysis},
  volume={50},
  pages={65--81},
  year={2018},
  publisher={Elsevier}
}
