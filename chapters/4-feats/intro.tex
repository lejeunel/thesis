\chapter{Deep Feature Learning}

\newcounter{algsubstate}
\renewcommand{\thealgsubstate}{\alph{algsubstate}}
\newenvironment{algsubstates}
  {\setcounter{algsubstate}{0}%
   \renewcommand{\State}{%
     \stepcounter{algsubstate}%
     \Statex {\footnotesize\thealgsubstate:}\space}}
  {}
\section{Introduction}

This chapter investigates several feature extraction methods as a contribution to the sparse point-wise annotation of the next chapter.

In section \ref{sec:cnn}, we give a brief introduction to Deep Convolutional Neural Networks (CNN) by starting from multi-layer perceptron (MLP).
In section \ref{sec:feat_method}, we introduce our experimental framework for the study of feature learning in the frame of our general problem of sparse point-wise annotation.
In particular, several baseline methods are introduced along the deep learning approach used in \cite{lejeune18}.
Extensive experiments are performed and results are given in section \ref{sec:feats_results}.

\section{Random Forest}
\label{sec:rf}
This section describes the \gls{rf} method, which we will use in the evaluation phase of our deep feature study.
Most of the following content is taken from \cite{hastie09}.

\gls{rf} is an algorithm that builds up from the idea of bootstrap aggregation (bagging).
Bagging consists in combining a commitee of weak learners (high variance and low bias estimators).
Combining such learners allows to decrease the variance one would get when using a single learner.
The \gls{rf} algorithm generates a single decision tree per bootstrap sample.
In the present experimental setup, our task is to predict the class of a given sample (foreground or background).
Once the set of trees are fitted, we infer the class probability of a given sample by computing the average vote over all trees.
The training algorithm is described in Alg. \ref{alg:rf}.

\begin{algorithm}[H]
  \label{alg:rf}
 \caption{Training a Random Forest for classification}
 \begin{algorithmic}[1]
 \Require{Input samples $\bm{X}$, $B$: Number of trees, $N$: Number of samples per tree, $m$: Number of variables to pick at each split, $n_{min}$: Minimum size of a non-leaf node}
  \Ensure{$\bm{T} = \{T_{b}^{i}\}_{i=1}^{N_{T}}$: Set of trees}
    \For{$b \gets 1$ to $B$}
        \State Draw a bootstrap sample $\bm{Z}^{*}$ of size $N$ from $\bm{X}$
        \State Grow a tree $T_{b}$ on $\bm{Z}^{*}$ by recursively repeating the following steps for each non-leaf node of the tree, until the minimum node size $n_{min}$ is reached
        \begin{algsubstates}
                \State Select $m$ variables at random from the $p$ variables
                \State Pick the best variable/split-point among the $m$
                \State Split the node into two children nodes
            \end{algsubstates}
    \EndFor
  \end{algorithmic}
\end{algorithm}


\endinput

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
