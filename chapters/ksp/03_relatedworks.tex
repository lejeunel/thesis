\section{Related Works}
\label{sec:related_works}
While semi-supervised methods for segmentation tasks encompass a wide range of applications and settings \cite{Chapelle2006}, we briefly discuss a number of methods that are related to the present paper. In particular, we focus on graph-based and learning-based methods, as well as methods that leverage different user-input mechanisms.

\noindent
{\bf Graph-based methods:} 
Graph-based methods are well studied in both the computer vision and medical imaging communities. The seminal work of \cite{boykov2006} first introduced an efficient and optimal method for binary segmentation using both object and background appearence models. GrabCut and other variants \cite{rother04,yu2014} further improved the approach using iterative optimizations. At their core, these methods rely on object and background models, computed from provided supervision, to segment objects. More recently, the approach of~\cite{karthikeyan13} extracts visual {\it tracklets} by combining gaze inputs from multiple individuals and optimizes a patchwork of locations using a Hungarian algorithm to globally extract bounding boxes that are then refined using GrabCut. In particular, by leveraging crowds of users to provide pointwise indications of object of interest, the method effectively produces segmentations from clouds of points. In contrast, in the approach we propose, only a single point per frame within the object of interest is given. This is similar to the work of~\cite{khosravan16}, who make use of saliency maps \cite{koch98} derived from gaze locations in CT scans to segment lung lesions. The saliency maps serve as object and background models (assuming bounds on the lesion sizes) in a graph cut optimizer.

\noindent
{\bf Semi-supervised learning methods:} A wide range of semi-supervised learning methods are related to our present work. Given that in our setting, only positive examples consisting of parts of the object are provided, our problem is closely related to transductive learning \cite{Burges13,Guyon17} and more specifically P(ositive)-U(nlabeled) learning \cite{Li2005,Kiryo2017}. In such cases,  only part of the positive set is labeled in addition to a large amount of unlabeled data. To tackle this setting, most methods focus on providing more adapted loss functions during training or leveraging priors to constrain the ensuing classifier.

Early on and also using a gaze tracker, \cite{vilarino07} suggested a P-U learning setting to detect polyps from endoscopic video frames. This approach bares some semblance to ours, except that we explicitly take into account temporal information by means of a graph to further constrain our segmentation. At the same time, unlike their approach, we do not assume that the object is of a given size. Along this line, \cite{lejeune17} considered a P-U setting by explicitly learning a classifier using a loss function that takes into account the uncertainty associated with unlabeled samples. These uncertainties are derived from gaze locations while Probability Propagation \cite{zhou04} is used to estimate unknown samples. Within a deep learning framework,~\cite{bearman16} suggested learning a CNN using gaze information as well as a strong object prior in order to improve convergence of their network. The method performs well on natural images of complex scenes, as the objectness prior is learned from a large corpus of natural images. Similarly, FusionSeg \cite{jain17} used a deep learning approach with an initial object outline to segment object boundaries in video sequences. This approach, which is highly related to tracking, combines both motion and appearance to track the object with limited user interaction.

\noindent
{\bf User-input models:} Given the wide use of machine learning, the extensive research on user-input methods and interactive algorithms, is by no means surprising. Beyond traditional polygon outlining, scribbling has been proposed to annotate faster. 2D point locations, either on individual images or in video streams has also been shown to be effective when providing coarse information in extremely fast amounts of time \cite{Papadopoulos17}.

Related to the work here, gaze trackers have received an increasing amount of attention given that the technology has greatly improved over the last decade and seen a strong reduction in cost \cite{soliman16,mettes16,bearman16}. In these works, gaze information provides a form of sparse annotations to train machine learning classifiers extremely quickly. In particular, large amounts of annotations can be accumulated by crowds of individuals observing natural video data for example. In the context of medical imaging, gaze locations have also been investigated to see how image annotation could be performed \cite{sadegh09}, or how pathologies could be identified by a limited number of viewings of video or volumetric image data \cite{vilarino07,khosravan16}. Unfortunately, most approaches so far have only been shown to work in extremely limited scenarios (\eg one type of object in a single modality). In our work, we show how object segmentations can be computed by using a gaze tracker to collect 2D locations of the object at framerate, in a single pass, without collecting or assuming information on the background scene, the object size or its motion speed. This allows our approach to be highly generic and effective on a variety of image modalities and object types.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:

