\section{Background Theory}

\subsection{Random Forest}
\label{sec:rf}
This section describes the \gls{rf} method, which we will use in the evaluation phase of our deep feature study.
Most of the following content is taken from \cite{hastie09}.

\gls{rf} is an algorithm that builds up from the idea of bootstrap aggregation (bagging).
Bagging consists in combining a committee of weak learners (high variance and low bias estimators).
Combining such learners allows to decrease the variance one would get when using a single learner.
The \gls{rf} algorithm generates a single decision tree per bootstrap sample.
In the present experimental setup, our task is to predict the class of a given sample (foreground or background).
Once the set of trees are fitted, we infer the class probability of a given sample by computing the average vote over all trees.
The training algorithm is described in Alg. \ref{alg:rf}.

\begin{algorithm}[H]
  \label{alg:rf}
 \caption{Training a Random Forest for classification}
 \begin{algorithmic}[1]
 \Require{Input samples $\bm{X}$, $B$: Number of trees, $N$: Number of samples per tree, $m$: Number of variables to pick at each split, $n_{min}$: Minimum size of a non-leaf node}
  \Ensure{$\bm{T} = \{T_{b}^{i}\}_{i=1}^{N_{T}}$: Set of trees}
    \For{$b \gets 1$ to $B$}
        \State Draw a bootstrap sample $\bm{Z}^{*}$ of size $N$ from $\bm{X}$
        \State Grow a tree $T_{b}$ on $\bm{Z}^{*}$ by recursively repeating the following steps for each non-leaf node of the tree, until the minimum node size $n_{min}$ is reached
        \begin{algsubstates}
                \State Select $m$ variables at random from the $p$ variables
                \State Pick the best variable/split-point among the $m$
                \State Split the node into two children nodes
            \end{algsubstates}
    \EndFor
  \end{algorithmic}
\end{algorithm}


\subsection{Deep Convolutional Neural Network}
\label{sec:cnn}

This chapter serves as an introduction to the basic notions that will come up in the remaining of this thesis, namely the notion of \gls{nn} and \gls{cnn}.
We start by introducing generalities about \gls{nn}.
Most of the content of the present chapter is taken from \cite{goodfellow16}.

\subsubsection{Neural Network}
Deep learning essentially refers to a family of machine learning methods that historically derive from the \gls{mlp}, also known as Neural Network.
An \gls{mlp} aims at approximating an unknown function $f^{*}$ with a function $f(x;\theta)$ parameterized by $\theta$.
More concretely, an \gls{mlp} converts an input $x$ to an output $y$ using a chain of function $f^{(1)} \circ f^{2} \circ \cdots \circ f^{n}(x)$, which justifies its name ``network''.
Each function is usually referred to as ``layer'', while the term ``neural'' comes from the fact that it is originally inspired by neuroscience \cite{mcculloch43}.

As in all machine learning setup, one wants to find the set of parameters $\bm{\theta}$ so as to optimize a cost (or loss) function, for example the \gls{mse}:

\begin{equation}
\mathcal{L}(\bm{\theta}) = \sum_{x\in \mathcal{X}}(f^{*}(x)-f(x;\bm{\theta}))
\end{equation}

One typically use a gradient-descent method to solve the above problem.

In the frame of \gls{mlp}, the functions $f^{m}(x;\bm{\theta_{m}})$ are linear (or fully-connected) layer, i.e. $\bm{\theta}_m=(\bm{w)}_{m},b$, followed by an activation function $\sigma(.)$

\begin{equation}
f^{m}(x;\bm{\theta_{m}}) = \sigma(\bm{x}^{T}\bm{w} + b)
\end{equation}

The activation function, allows to inject non-linearities in the model.
Such function is applied element-wise.
The \gls{relu} is the most widespread.
It writes:

\begin{equation}
\sigma(x) = \max \{0, \bm{x}\}
\end{equation}

\subsubsection{Convolutional Network}
Many machine learning applications rely on structured data, i.e. data that respect a grid-like topology.
Typical examples include time-series and images.
For this reason, \cite{lecun95} introduced the notion of convolutional network, which adapts the previous MLP to leverage the grid-like topolgy in an effective manner.
In particular, the author replaces one or several layers of an MLP with convolutional layers.
In the case of (discrete) images, the convolution operator writes:

\begin{equation}
S(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(m,n) K(i-m, j-n)
\end{equation}

where $I$ is a grayscale image and $K$ is a kernel.
This formulation naturally extends to multi-channel images.
The convolutional operator brings the following advantage over the \gls{mlp}: (1) It naturally leverages the spatial connectivity of pixels.
(2) As the size of the kernel is usually much smaller than the input size (sparse-connectivity), the number of parameters are greatly reduced, which reduces the memory consumption, accelerates the training and improves performances \cite{lecun95}.
Fig. \ref{fig:cnn_con} illustrates the idea of sparse connectivity brought by convolutional layers.

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=10cm]{fc_vs_conv}
  \caption{(Left) A fully-connected (linear) layer with 1D input at the bottom. (Right) Convolutional layer with a kernel of size 3.}
  \label{fig:cnn_con}
\end{figure}

\subsubsection{Pooling}

Another important component of Convolutional networks are pooling layers, which allow to increase the receptive field of the convolutional operators as the depth is increased.
This effectively allow to capture, in the case of images, visual features at different spatial scales, i.e. that gets more global as the depth increases.
In particular, the max-pooling operator computes for each spatial location the maximal filter response over a pre-defined neighborhood (see fig. \ref{fig:max_pool})

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=10cm]{max_pooling}
  \caption{(Left) Input. (Right) Output of max-pooling layer. Each color represents a set of values included in the sliding window.}
  \label{fig:max_pool}
\end{figure}
Figure \ref{fig:cnn} illustrates a CNN applied to an image processing task.

\subsubsection{Optimization}

\begin{algorithm}
  \label{alg:sgd}
 \caption{Stochastic Gradient Descent (SGD)}

 \begin{algorithmic}[1]
  \Require{Training data ${(\bm{x}_{i},y_{j})}_{i=1}^{N}$, learning rate $\lambda$, initial parameters $\bm{\theta}$}
  \Ensure{Model parameters $\bm{\theta}$}
  \Repeat
    \State Sample a minibatch of m examples from the training set $\{(\bm{x}_{1},y_{1}), \cdots, (\bm{x}_{m},y_{m})\}$
    \State Compute gradient estimate: $\hat{\bm{g}} \leftarrow + \frac{1}{m} \nabla_{\bm{\theta}}\sum_{i}\mathcal{L}(f(\bm{x}_{i}; \bm{\theta}), y_{i})$
    \State Apply gradient update: $\bm{\theta} \leftarrow \bm{\theta} - \lambda \hat{\bm{g}}$
    \Until {stopping criterion is met}
  \end{algorithmic}
\end{algorithm}

Since the advent of deep learning, many optimization algorithms have been proposed and proved to be effective, each with their own pros and cons.
To name a few: \gls{sgd}, Adam \cite{kingma14}, and RMSProp \cite{tieleman12}.
As the two latter are essentially improvements of \gls{sgd}, we restrict this section to \gls{sgd} (see Alg. \ref{alg:sgd}).
In practice, effectively training a deep learning model demands careful study of hyper-parameters.
The most crucial is certainly the learning rate $\lambda$ which controls the fraction of gradient-estimate one wants to use to update $\bm{\theta}$.
Also, one needs to initialize $\bm{\theta}$ to random values.
The latter step has been an important object of study, e.g \cite{he15}, \cite{glorot10}.



\subsubsection{Batch normalization}

\begin{algorithm}
  \label{alg:batchnorm}
 \caption{Batch Normalization}
 \begin{algorithmic}[1]
  \Require{Minibatch of size $M$: $\mathcal{B}=\{(\bm{x}_{i})\}_{i=1}^{M}$, learnable scaling and shifting parameters, $\gamma$ and $\beta$}
  \Ensure{$\bm{x}'_{i}=\text{BN}(\bm{x}_{i};\gamma,\beta)$}
    \State Compute mean: $\mu_{\mathcal{B}}\leftarrow \frac{1}{m}\sum_{i=1}^{M}\bm{x}_{i}$
    \State Compute variance: $\sigma^{2}_{\mathcal{B}}\leftarrow \frac{1}{m}\sum_{i=1}^{M}(\bm{x}_{i}-\mu_{\mathcal{B}})$
    \State Normalize: $\hat{\bm{x}}_{i}\leftarrow \frac{\bm{x}_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma^{2}_{\mathcal{B}} + \epsilon}}$

    \State Scale and shift: $x_{i}'\leftarrow \gamma \hat{\bm{x}}_{i} + \beta \equiv \text{BN}(\bm{x}_{i};\gamma,\beta)$
 \end{algorithmic}
\end{algorithm}


As already mentioned, training a CNN demands a careful tuning of the learning rate in order to converge to a proper solution.
Practical issues such as exploding or vanishing gradient arise when the latter is two high or too low, respectively.
In particular, as noted in \cite{ioffe15}, the input distribution of a given layer is dependent on the parameters of all preceding layers, a phenomenon called internal covariate shift.
To circumvent the latter problem, a batch normalization can be added at the output of each layer to normalize the values of a given minibatch to a normal distribution.
Additionally, such normalization often needs to combine with an activation function, and therefore will have its left tail zeroed-out in the case of a \gls{relu}.
To circumvent that, authors add learnable scaling and shifting parameters, $\gamma$ and $\beta$, respectively.
Also, batch normalization also has a regularization effect due to fluctuations in mini-batch statistics \cite{gastaldi17}.
The algorithm is described in Alg. \ref{alg:batchnorm}.

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=13cm]{cnn}
  \caption{Convolutional Neural Network applied to an image task.
    The network applies a succession of convolution, and pooling (subsampling) operation.
  Activation functions are not represented. Figure taken from \cite{lecun95}}
  \label{fig:cnn}
\end{figure}

\subsubsection{Convolutional Autoencoder}
The basic idea of \gls{ae}, first introduced in \cite{vincent10}, consists in training a model in an unsupervised manner, so as to generate a compressed representation of the input.
Such compressed representation, often called feature vector, is typically used in a subsequent machine learning task.
In particular, an \gls{ae} is composed of two modules: An encoding function $f_{\theta}$, and a decoding function $g_{\phi}$.
The training objective is to minimize the discrepancy between an input sample $x$ and its reconstructed version $x'$.
An example is shown on Fig. \ref{fig:ae}.

For example, using the \gls{mse} criterion, one writes:

\begin{equation}
\mathcal{L}_{AE} = \mathbb{E}_{x_{i}}||f_{\theta}(g_{\phi}(x)) - x ||
\end{equation}


\begin{figure}[!htpb]
  \centering
  \includegraphics[width=7cm]{ae}
  \caption{Convolutional Autoencoder.
    The input sample $x$ passes through an encoder.
    The output of the encoder, called $h$, passes through a decoder to give the reconstructed version $x'$.
    In yellow is the bottleneck.}
  \label{fig:ae}
\end{figure}

\section{Multi-path tracking}
This section gives a theoretical background to the multi-path tracking problem, as first developed in \cite{berclaz11}.
The latter framework is leveraged in our first major contribution, where we perform tracking of over-segmented regions across a sequence, where each region can be part of object of interest.

We develop our multi-path tracking framework through the following steps.
(1) We first represent our sequence as a stack of occupancy grid, where each element of a grid represent an over-segmented region.
The elements of these grids are then represented as the nodes of a directed acyclic graph, connected by edges that represent admissible motions.
(2) We introduce the notion of discrete flow variables, which represent the number of objects passing from one position of the grid to another.
(3) We formulate the multi-path tracking problem as a \gls{map} problem, where the grid occupancies are binary random variables.
(4) We show how the latter problem, under the Markov assumption, is identical to an \gls{ip}.
(5) As the latter is NP hard, we show that relaxing it to a continuous \gls{lp} and solving it using off-the-shelf solvers converges to the optimal solution.
(6) We further show that the implicit spatio-temporal relations of our occupancy grid allow to leverage the \gls{ksp} algorithm.

Formally, we start by formulating our problem in the network-flow paradigm.
Next, we show the latter allow to solve a \gls{map} problem, where the objectness of over-segmented regions are the variables to optimize, and show how the likelihoods can be modeled by appearance similarity models.
Last, we show how solving the latter \gls{map}, when cast into a network flow problem, can be solved efficiently.

\subsection{Segmentation as a network flow problem on over-segmented regions}
We depart from the work of \cite{berclaz11} on two aspects: (1) In contrast to the latter authors, who represent a sequence as a stack of coarse grids, i.e. each cell represent a physical position and the relative locations are fixed in advance, we rather consider that our sequence is over-segmented into superpixels.
(2) We leverage the ``tracklet'' paradigm, where each over-segmented region is represented as a short track in which flow is allowed to pass through.


In particular, we generate on each frame, indexed by a time variable $t$, a set of $N_{t}$ non-overlapping over-segmented regions $\mathcal{S}_{t}=\{s_{t}^{n}\}_{n=1}^{N_{t}}$.
Within the directed graph representation, we represent each over-segmented region $s^{t}_{n}$ by a couple of nodes connected by a \textit{visiting} edge $e^{n}_{t}$.
As a side note, the latter is often referred to as a ``tracklet'' \cite{zhang08}.
So as to allow objects to transit from one frame to the next, we add \textit{transition} edges $e^{(n,m)}_{t}$ that connect region $s^{n}_{t}$ to region $s^{m}_{t+1}$.
Each edge is labeled with a discrete non-negative flow variable.
In particular, $f_{t}^{n}$ corresponds to the flow transiting through edge $e_{t}^{n}$, while $f_{t}^{n,m}$ corresponds to the flow passing from region $s_{t}^{n}$ to region $s_{t+1}^{m}$.
Next, we impose conservation of flow, which imposes that the quantity of flow that passes into a visiting edge is equal to the quantity of flow that leaves it. Formally:

\begin{equation}
  \label{eq:flow_conserv}
  \forall t,n \quad f_{t}^{n} = \sum_{n:m\in \mathcal{N}(n)}f_{t}^{n,m}
\end{equation}

When $\mathcal{N}(n)$ is a spatial neighborhood centered on region $n$ that defines admissible motion.

Next, as we assume that each region can contain a maximum of one object. Formally:

\begin{equation}
  \label{eq:capa_constrain}
  \forall t,n \quad f_{t}^{n} \leq 1
\end{equation}

Note that Eq. \ref{eq:flow_conserv} and \ref{eq:capa_constrain} implicitly enforce a maximum flow constraints on transition edges.

At the root of our segmentation framework lies the basic idea that our object to segment is composed of over-segmented regions that are spatially and temporally organized.
Moreover, we expect that the size of our object of interest changes through time/slice, e.g. a brain tumor on transversal slices appears to grow and shrink again as one scrolls from one end of the scan to the other end.
To address this requirement, we introduce two kinds of \textit{virtual} nodes: A source node and a sink node.
The first acts like a tap, i.e. it pushes flow inside the graph and increases the total mass, while the second allows to evacuate mass.
We ensure a flow-mass conservation through the constraint:

\begin{equation}
  \label{eq:mass_constrain}
  \sum_{t,m\in \mathcal{N}(\xi_{t})}f_{xi_{t},m} = \sum_{k:\mathcal{X}\in\mathcal{N}(k)}f^{t}_{k,\mathcal{X}}
\end{equation}

Where the $\xi_{t}$ are proxy source nodes that allow pushing flow on frame $t$, and $\mathcal{X}$ is the sink node.
Fig. \ref{fig:flownetwork} illustrates our network flow.

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=13cm]{network}
  \caption{Illustration of our flow network. Each over-segmented region is assigned a visiting edge (in blue).
    Flow is allowed to pass from one frame to the next through the transition edges (in green).
    A set of proxy source nodes $\xi_{t}$ allow to push flow into the network through entrance edges (in red).
  The sink node $\mathcal{X}$ allows to evacuate flow through exit edges (in orange).}
  \label{fig:flownetwork}
\end{figure}

\subsection{Maximum a Posteriori formulation}
We first formulate an \gls{map} estimation problem, where the random variable to optimize for denotes the presence of objects at discrete time-space locations.
Next, we emphasize how the network flow paradigm developed above is leveraged in order to solve the \gls{map} estimation problem.

Let $\bm{Y}={Y^{n}_{t}|\forall(t,n)}$ a set of binary random variable that take value $1$ when region $s^{n}_{t}$ is object, and $0$ otherwise, while $\bm{I}_{t=1}^{N}$ and $\bm{g}_{t=1}^{N}$ are a set of $N$ images and user-provided 2D locations, respectively.
We define our segmentation task as.

\begin{equation}
  \label{eq:map}
  y^{*} = \arg \max_{y \in \mathcal{Y}}P(\bm{Y}=\bm{y}|\bm{I}, \bm{g})
\end{equation}

where $y^{*}$ are the optimal binary labels.
Assuming that $Y^{t}_{n}$ is conditionally independent given the observed variables, we rewrite Eq. \ref{eq:map} as

\begin{equation}
  \label{eq:bg_map2}
  y^{*} = \arg \max_{y \in \mathcal{Y}}\prod_{i,j,t} P_{visit}(Y^{t}_{n}|\bm{I}, \bm{g}) P_{trans}(Y^{t}_{n}|I^{t-1},\bm{g}) P_{in}(Y^{t}_{n}|I^{t},\bm{g})
\end{equation}

The decomposition of \ref{eq:bg_map2} shows three different likelihood models that will provide the costs of blue, red, and green edges of Fig. \ref{fig:flownetwork}, respectively.
Next, we want to convert Eq. \ref{eq:bg_map2} to a linear sum of its random variables.
To this end, we provide the following lemma:

\begin{lemma}
  \label{lm:linprog}
  Let $y^{*}=\arg\max_{y\in\mathcal{Y}}P(Y^{t}_{n}|\bm{x})$ an \gls{map} problem to solve, and $\rho^{t}_{n}=P(Y^{t}_{n}=1|\bm{x})$ the marginal posterior probability that $Y^{t}_{n}$ contains an object.
  The latter \gls{map} problem can be rewritten as a linear expression of the binary random variables through:

  \begin{align}
    y^{*} &= \arg \max_{y\in\mathcal{Y}} \quad \log \prod_{t,n} P(Y^{t}_{n}=y^{t}_{n}|\bm{x})\\
          &= \arg \max_{y\in\mathcal{Y}} \sum_{t,n} \log P(Y^{t}_{n}=y^{t}_{n}|\bm{x}) \\
    &= \arg \max_{y\in\mathcal{Y}} \sum_{t,n} (1-y^{t}_{n})\log P(Y^{t}_{n}=0|\bm{x}) + y^{t}_{n}\log P(Y^{t}_{n}=1|\bm{x}) \label{eq:lmbinary}\\
    &= \arg \max_{y\in\mathcal{Y}} \sum_{t,n} y^{t}_{n}\log \frac{P(Y^{t}_{n}=1|\bm{x})}{P(Y^{t}_{n}=0|\bm{x})} \label{eq:lmignore}\\
    &= \arg \max_{y\in\mathcal{Y}} \sum_{t,n} y^{t}_{n}\log \frac{\rho^{t}_{n}}{1-\rho^{t}_{n}}
  \end{align}

Where Eq. \ref{eq:lmbinary} is true because of the binarity of the random variables, and Eq. \ref{eq:lmignore} is obtained by ignoring a term independent on $\bm{y}$.
\end{lemma}

\subsection{Linear Programming Formulation}
Our original \gls{map} problem of Eq. \ref{eq:map} can now be rewritten as an \gls{ip} thanks to lemma \ref{lm:linprog}.
In particular, we let our binary random variables $y^{t}_{n}=1$ when the flow variables $f^{t}_{n}=1$, and $0$ otherwise.
Our \gls{ip} writes:

\begin{subequations}
\label{eq:bg_int_prog}
\begin{align}
\intertext{Maximize}
&\sum_{t,n} \log{\frac{\rho_n^t}{1-\rho_n^t}}f_n^t + \sum_{t,m} \log{\frac{\alpha_{m,n}^{t}}{1-\alpha_{m,n}^{t}}}\sum_{t,n}f_{m,n}^{t} + \sum_{t,n} \log{\frac{\beta_n^t}{1-\beta_n^t}}f_{\mathcal{E}_t,n}^{t},\label{eq:bg_loglikelihood}\\
\intertext{subject to,}
&f_{m,n}^{t} \geq 0, \qquad \forall t,m,n \label{eq:nonneg_flow}\\
&\sum\limits_{n}f_{m,n}^{t} \leq 1, \qquad \forall t,m,n \label{eq:bg_cap1_trans}\\
&\sum_m f_{m,n}^{t} - \sum_p f^{t-1}_{p,m} \leq 0, \qquad \forall t,m,n,p \label{eq:bg_conserv1}\\
&\sum_{m,t} f^t_{\mathcal{E}_t,m} - \sum_p f_{p,\mathcal{X}} \leq 0, \qquad \forall t,m\label{eq:bg_conserv2}
\end{align}
\end{subequations}

As mentioned in \cite{berclaz11}, the above \gls{ip} is NP-complete, which prohibits the use of off-the-shelf \gls{lp} solvers.
To circumvent that, authors suggest to relax the \gls{ip} into an \gls{lp}.
In particular, the problem of Eq. \ref{eq:bg_int_prog} is converted to its \textit{canonical form}, by aggregating Eq. \ref{eq:bg_cap1_trans}, \ref{eq:bg_conserv1}, and \ref{eq:bg_conserv2} into a constraint matrix $C$ such that

\begin{equation}
  C \cdot \bm{f} \leq [1, \ldots, 1, 0, \ldots, 0]^{T}
\end{equation}
Thanks to the  \textit{total unimodularity} of the constraint matrix, the \gls{lp} converges to integer solutions.
We kindly redirect the reader to the proof given in \cite{berclaz11}.

\section{Recursive Bayesian Parameter Estimation}
In this section, we provide a theoretical background to recursive bayesian parameter estimation as a complement to our last contribution.
In particular, this contribution relies on estimating class-priors on every frame of a sequence, i.e. estimating the proportion of positive over-segmented regions.

Here, we start by formulating the state-estimation problem using the hidden Markov model framework.
Next, we show how the traditional Kalman filtering algorithm \cite{kalman1960} allows to estimate a posteriori state-estimates using observations in an efficient manner when specific assumptions are validated.
We then elaborate on the \gls{ekf} and its successor, the \gls{ukf}

\subsection{Hidden Markov model}
In recursive bayesian parameter estimation, we aim at estimating the true value of a state variable $\bm{x}$ over time, by leveraging incoming noisy observations $\bm{z}$.
We assume that a mathematical model exists that evolve the current true value of the state to the state at the previous time-step.
Furthermore, another model provides the relation between the observation of and the state.
Formally, we define the following discrete-time dynamic system:

\begin{align}
  \bm{x}_{k+1}&=F(\bm{x}_{k},\bm{v}_{k}) \label{eq:bg_state_trans}\\
  \bm{z}_{k}&= H(\bm{x}_{k}, \bm{n}_{k}) \label{eq:bg_proc}
\end{align}

where $\bm{x}$ denotes the state variable, $\bm{z}$ denotes the observations,
$F$ and $H$ are the transition and process models, respectively, while $\bm{v}$ and $\bm{n}$ are the process and transition noise, respectively.
Fig. \ref{fig:hmm} represents the dynamic system of Eq. \ref{eq:bg_state_trans} and \ref{eq:bg_proc} as a directed graph, where edges represent probabilistic relationships.
In the recursive bayesian filtering paradigm, we assume that all variables are stochastic.
Furthermore, the systems follows the Markov assumption, i.e. every variable is conditionally independent of its non-descendants, given its parents \cite{geiger90}.
Formally,

\begin{align}
  p(\bm{x}_{k+1}|\bm{x}_{k}, \bm{x}_{k-1},\ldots,\bm{x}_{0})=p(\bm{x}_{k+1}|\bm{x}_{k})\\
  p(\bm{z}_{k}|\bm{x}_{k}, \bm{x}_{k-1},\ldots,\bm{x}_{0})=p(\bm{z}_{k}|\bm{x}_{k})
\end{align}

When the state variables are unobserved, but is dependent on another observable process, we call such a system a \gls{hmm}.

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=10cm]{hmm}
  \caption{Graphical representation of a \gls{hmm}. $x$ are the hidden variables, while $z$ are the observations.
  Edges represent probabilistic relationships.}
  \label{fig:hmm}
\end{figure}

\subsection{Recursive bayesian filtering and Kalman Filter Equations}
In recursive bayesian filtering, we are interested in estimating the \gls{pdf} of the current state using the history of observations up to the current time-step, written $p(\bm{x}_{k}|\bm{z}_{0:k}$, where $\bm{z}_{0:k}$ is a short form for $\bm{z}_{0},\ldots, \bm{z}_{k}$.

Using Baye's rule, we rewrite:
\begin{equation}
  \label{eq:bg_aposteriori}
  p(\bm{x}_{k}|\bm{z}_{0:k}) = \frac{p(\bm{x}_{k}|\bm{z}_{0:k-1})\cdot p(\bm{x}_{k}|\bm{z}_{k})}{p(\bm{x}_{k}|\bm{z}_{0:k-1})}
\end{equation}

The left term of the denominator of Eq. \ref{eq:bg_aposteriori} is called the a-priori probability distribution of the current state.
We rewrite it in a recursive form as
\begin{equation}
  \label{eq:bg_prior}
  p(\bm{x}_{k}|\bm{z}_{0:k-1}) = \int p(\bm{x}_{k}|\bm{x}_{k-1}) \cdot p(\bm{x}_{k-1}|\bm{z}_{0:k-1}) d\bm{x}_{k-1}
\end{equation}

Next, the denominator is a normalization term written:
\begin{equation}
  \label{eq:bg_norm_cst}
  p(\bm{z}_{k}|\bm{z}_{0:k-1}) = \int p(\bm{x}_{k}|\bm{z}_{0:k-1}) \cdot p(\bm{z}_{k}|\bm{x}_{k}) d\bm{x}_{k}
\end{equation}

In this general form, the exact inference of Eq. \ref{eq:bg_aposteriori} is untracktable.
However, many algorithms exists that solve this problems approximately under specific conditions, e.g. Expectation-Maximization \cite{dempster77} and Markov-chain Monte Carlo \cite{geyer92}.

\subsubsection{Kalman Filter}

Under the more restrictive assumptions that (1) models $F$ and $G$ are linear functions, (2) the initial values of the state variables follow a multivariate normal distribution, and (3) the process and observation noise are additive, zero-mean and normally distributed, the \gls{kf} algorithm provides exact solutions.
This comes from the fact that under the above conditions, the a-posteriori pdf \ref{eq:bg_aposteriori} is also normally distributed, and is therefore fully characterized by its first and second moment, i.e. its mean and covariance.
Under these conditions, the condition \gls{pdf} is fully characterized by its first and second moment, i.e. mean and covariance.

the \gls{kf} only needs to propagate the
Under these assumptions, the state-space model is rewritten :

\begin{align}
  \bm{x}_{k+1}&=F_{k}\bm{x}_{k} B_{k}u_{k} + \bm{v}_{k} \label{eq:bg_state_trans_kf}\\
  \bm{z}_{k}&= H_{k}\bm{x}_{k} + \bm{n}_{k} \label{eq:bg_proc_kf} \\
  \bm{x}_{0}&= \bm{\bar{x}}_{0} + \mathcal{N}(0,S) \label{eq:bg_init_kf}
\end{align}

Where $\bm{v}_{k}=\mathcal{N}(0,Q)$, $\bm{n}_{k}=\mathcal{N}(0,R)$ are the process and observation noise signals, respectively, while $Q$, $R$, and $S$ are the process, observation and initial covariance matrices, respectively.

\subsubsection{Kalman Filter inference}
We now provide an overview of the main steps of the \gls{kf} algorithm.
For the detailed derivations, we refer the reader to \cite{thacker98}.

The problem of \gls{kf} is to produce an optimal estimate of the hidden state variable, written $\bm{\hat x}_{k}$, given the observations.
In particular, it takes as criteria of optimality the \gls{mse} between the estimate and the true value of the state.
As shown in \cite{ribeiro04}, the estimator that minimizes this criteria is the the conditional mean, i.e.

\begin{equation}
  \label{eq:bg_cond_mean}
  \bm{\hat x}_{k} = \mathbb{E}[\bm{x}_{k}|\bm{z}_{0:k}]
\end{equation}

The inference is decomposed into a prediction and filtering step, i.e.

\begin{itemize}
    \item \textbf{Prediction: } $p(\bm{x}_{k}|\bm{z}_{0:k}) \rightarrow p(\bm{x}_{k+1}|\bm{z}_{0:k})$
    \item \textbf{filtering: } $p(\bm{x}_{k+1}|\bm{z}_{0:k}) \rightarrow p(\bm{x}_{k+1}|\bm{z}_{0:k+1})$
\end{itemize}

Where the prediction step applies the transition function to the current state estimate, thereby computing the a-priori conditional \gls{pdf}, and the filtering corrects the latter using the newly arrived observation $\bm{z}_{k+1}$, thereby computing the a-posteriori conditional \gls{pdf}.

Formally, the prediction step resolves to the following recursive equation:

\textbf{Prediction:}
\begin{align}
  \bm{\hat x}^{-}_{k+1}&=F_{k} \bm{\hat x}_{k} + B_{k}u_{k} \label{eq:bg_apriori_estim}\\
  P_{k+1}^{-}&=F_{k} P_{k} F_{k}^{T} + Q_{k} \label{eq:bg_apriori_cov}
\end{align}

Where $P_{k}$ is the covariance matrix of the state, and $\bm{\hat x}^{-}$ denotes the a-priori state estimate.
Eq. \ref{eq:bg_apriori_estim} computes the a-priori state estimate, and Eq. \ref{eq:bg_apriori_cov} computes the a-priori state covariance.

\textbf{Correction:}
\begin{align}
  \bm{\tilde y}_{k+1} &= \bm{z}_{k+1} - H_{k+1} \bm{\hat x}_{k+1}^{-} \label{eq:bg_innov} \\
  T_{k+1} &= H_{k+1} P_{k+1}^{-} H_{k+1}^{T} + R_{k} \label{eq:bg_innov_cov} \\
  K_{k+1} &= P_{k+1}^{-} H_{k+1}^{T} T_{k+1}^{T} \label{eq:bg_kf_gain}\\
  \bm{\hat x}_{k+1} &= \bm{\hat x}_{k+1} + K_{k} \bm{\tilde y}_{k+1} \label{eq:bg_aposteriori_state_estim}\\
  P_{k+1} &= (I - K_{k+1}H_{k+1}) P_{k+1}^{-} \label{eq:bg_aposteriori_cov_estim}
\end{align}

Eq. \cref{eq:bg_innov,eq:bg_innov_cov,eq:bg_kf_gain,eq:bg_aposteriori_state_estim,eq:bg_aposteriori_cov_estim} compute the state innovation, covariance innovation, Kalman gain, a-posteriori state estimate, and a-posteriori covariance estimate, respectively.
In practice, the two above steps are applied each time a new observation arrives.

\subsection{Unscented Kalman Filter}
The standard \gls{kf} framework allows to compute the a-priori state estimate, Kalman gain, and a-posteriori state estimate \textit{exactly}, by propagating the mean and covariance of the state, two quantities that fully characterize its \gls{pdf}, through the linear-system dynamics.
However, when the system is non-linear, the \gls{kf} is prohibited, since the a-priori and a-posteriori conditional \gls{pdf}, after transformation, become non-gaussian.
The \gls{ekf} \cite{ribeiro04} attempts to generalize \gls{kf} to such scenario by approximating the moments of these \gls{pdf} using first-order Taylor expansion around the current estimates.
We choose not to delve into the details of \gls{ekf}, and simply mention that the latter approximation, in practice, bring sub-optimal results and often lead to the divergence of the filter \cite{wan00}.

We now give an overview of the \gls{ukf}, which attempts to amend the flaws of \gls{ekf}.

\subsubsection{Unscented Transformation}
\gls{ukf} considers the \gls{ut} \cite{julier96}, which discretize a (continuous) \gls{pdf} into a set of carefully chosen points so as to (1) capture the true mean and covariance accurately, and (2) capture the true posterior statistics up to the third order after propagation through non-linear functions.
This approach departs from \gls{ekf} in that, the non-linear functions are applied to these transformed points directly.

In particular, let $\bm{x}$ a \gls{grv} of dimension $L$ with mean $\bm{\bar x}$ and covariance $P_{x} \in \mathbb{R}^{L \cdot L}$.
The goal is to find a set of $2L$ from the rows or columns of the matrices $\pm \sqrt{L P_{x}}$ with zero mean and sample covariance $P$, and apply a translation of each point to get a mean of $\bm{\bar x}$.
We let $\bm{\mathcal{X}}$ a matrix whose rows contain $2L+1$ sigma-vectors $\bm{\mathcal{X}}_{i}$, and $\mathcal{Y} = g(\mathcal{X})$ its transformation trough an arbitrary function $g$.
The procedure is \cite{wan00}:
\begin{enumerate}
  \item Compute sigma-points $\mathcal{X}$ and corresponding weight matrix $W$ as:
    \begin{itemize}
      \item $\mathcal{\bm{X}}_{0} = \bm{\bar x}$
      \item $\mathcal{\bm{X}}_{i} = \bm{\bar x} + (\sqrt{(L + \lambda) P_{x}})_{i} \quad i=1,\ldots,L$
      \item $\mathcal{\bm{X}}_{i} = \bm{\bar x} - (\sqrt{(L + \lambda) P_{x}})_{i} \quad i=L+1,\ldots,2L+1$
      \item $W_{0}^{(m)}=\lambda / (L + \lambda)$
      \item $W_{0}^{(c)}=\lambda / (L + \lambda) + (3 - \alpha^{2})$
      \item $W_{i}^{(m)}=W_{i}^{(c)}= 1 / [2(L + \lambda)]$
    \end{itemize}
\end{enumerate}

Where $\lambda = L (\alpha^{2}-1)$ is a scaling parameter, and $\alpha$ determines the spread of sigma points around the mean.
Next, the sigma-points are propagated through $g$:
\begin{equation}
  \mathcal{Y}_{i} = g(\mathcal{X}_{i}) \quad i=0,\ldots,L
\end{equation}
Finally, we obtain the statistics of the output through the following approximations:
\begin{align}
  \bm{\bar y} & \approx \sum_{i=0}^{2L} W_{i}^{(m)}\mathcal{Y}_{i} \\
  P_{y} & \approx \sum_{i=0}^{2L} W_{i}^{(c)}[\mathcal{Y}_{i}-\bm{\bar y}][\mathcal{Y}_{i}-\bm{\bar y}]^{T}
\end{align}

\begin{figure}[!htpb]
  \centering
  \includegraphics[width=10cm]{ukf}
  \caption{Example of the Unscented Transformation and its mean and covariance after the propagation step.
  (a) Monte Carlo sampling (b) Extended Kalman Filter (linearized) (c) Unscented Transformation (UKF). Taken from \cite{wan00}.}
  \label{fig:ukf}
\end{figure}

Fig. \ref{fig:ukf} illustrates three strategies where a \gls{pdf} is propagated through a non-linear function, where the first samples arbitrary points, the second performs a linearization of the function as in \gls{ekf}, and the third follows the above \gls{ut} strategy.

The full procedure for \gls{ukf} is identical to the original \gls{kf}, i.e. performs
a prediction step (as in \cref{eq:bg_apriori_estim,eq:bg_apriori_cov}), and
a correction step (as in \cref{eq:bg_innov,eq:bg_innov_cov,eq:bg_kf_gain,eq:bg_aposteriori_state_estim,eq:bg_aposteriori_cov_estim}) by applying the above sigma-points sampling and propagation step.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
