\section{Experiments}
\label{sec:exps}
\subsection{Datasets}
To validate our methods, we evaluate on the publicly available dataset used in \cite{lejeune18}.
This consists in a variety of video and volumes of different modalities with 2D annotation points for different object types.
It includes:
\begin{itemize}
\item \textbf{Brain:} \(4\) 3D T2-weighted MRI scans chosen at random from the publicly available BRATS challenge dataset \cite{menze15}, where tumors are object of interest.
\item \textbf{Tweezer:} \(4\) sequences from the training set of the publicly dataset MICCAI Endoscopic Vision challenge: Robotic Instruments segmentation \cite{endochal}. The piecewise-rigid surgical instrument must be segmented.
\item \textbf{Slitlamp:} \(4\) slit-lamp video recordings of human retinas, where the optic disk is to be segmented.
\item \textbf{Cochlea:} \(4\) volumes of 3D CT scans of the inner ear, where the cochlea must be annotated. This object is the most challenging object to segment due to its complicated geometry.
\end{itemize}

\subsection{Proposed methods and baselines}
\label{sec:orge9e46ac}
\subsubsection{Baselines}
\label{sec:orgb4303b5}

We now evaluate the segmentation performance of the proposed foreground models with the following baselines.
\begin{itemize}
\item \ksp: Multi-object tracking method described in \cite{lejeune18}.
The foreground model is a bagging of decision trees similar to Random Forest.
\item \textbf{EEL}: An expected exponential loss to learn robust classifiers in a PU learning setting \cite{lejeune17}.
\item \textbf{Gaze2Segment}: A combination of saliency detection and graphcut \cite{khosravan16}.
\item \textbf{DL-prior}: Point location supervision is used to train a CNN with strong object priors \cite{bearman16}.
\end{itemize}

\subsubsection{Proposed}
\label{sec:org83c59ba}
\begin{itemize}
\item \puss: Non-negative positive-unlabeled (nnPU) with self-supervised class-prior estimation, as in Sec. \ref{sec:nnpu} and \ref{sec:pi_estim}.
\item \kspss: Combines the \puss method and the multi-object tracking refinement step of Sec. \ref{sec:tracking}.
\item \ksptrue: Same as above, except that the foreground model is the proposed CNN trained as in Alg. \ref{alg:sgdnnpu} using the groundtruth class-priors.
\item \kspconst: Same as above except we train Alg. \ref{alg:sgdnnpu} taking as prior a sequence-wise constant value given by the mean groundtruth prior over all frames.
\end{itemize}
For both \ksptrue{} and \kspconst{}, we train the foreground prediction model for $150$ epochs as in Alg. \ref{alg:sgdnnpu}. The learning rate is set to $10^{-4}$ and reduced to $10^{-5}$ after $50$ epochs.

\subsection{Segmentation performance}

We evaluate quantitatively the proposed methods (\puss{} and \kspss{}) to the baselines.
We set the initial upper-bound using the groundtruth annotations.
In particular, we compute for all frames the frequencies of positives $\pi^{i}$ and set $\hat \pi_{0}=1.4 \cdot \max_{i}{\pi^{i}}$.
As performance criteria, we use the F1 score, averaged on all $4$ sequences of a given type.
Quantitative results are shown in Tab. \ref{tab:results}.
Qualitative results are shown in Fig. \ref{fig:prevs}.

The \kspss{} method brings substantial improvement on the best baseline (\ksp{}) on all image modalities.
In particular, we report improvements in F1 scores of $18\%$ on Tweezer, $13\%$ on Cochlea, $9\%$ on Slitlamp, and $8\%$ on Brain.
Also, comparing \puss{} with \kspss{}, we quantify the added value of the proposed tracking module.
We report an improvement in average F1 score of $4\%$ on Tweezer, $41\%$ on Cochlea, $7\%$ on Slitlamp, and $6\%$ on Brain.

\input{chapters/nnpu/tables/results.tex}

\begin{figure*}[t]
\caption{Qualitative results for each type. From left to right: Original image with groundtruth highlighted in red and 2D location in green, output of foreground prediction model (nnPUss), thresholded output of the latter, and segmentation using the full pipeline (KSPTrack/nnPUss).}
\centering
    \includegraphics[width=.6\textwidth]{pics/prevs.png}
\label{fig:prevs}
\end{figure*}

\subsection{Analysis of Convergence}
\label{sec:convergence}

Fig. \ref{fig:converge} shows for each type of sequence the convergence of our class-prior estimation procedure (Sec. \ref{sec:pi_estim}).
All sequences are trained using $\hat\pi_{0}= 1.4 \cdot \max_i{\pi^i}$.

As we aim at estimating the true class-prior, we plot on the top panels the Mean Absolute Error (MAE) between the estimated priors and the true priors.
The proposed stopping condition, which leverages the variance of pseudo-negatives, is plotted in the bottom panel.

We observe that the proposed stopping condition triggers reasonably close to the optima in most cases.


\begin{figure*}[t]
\caption{For each type, we show: (top) Mean Absolute Error (MAE) between the predicted positive frequency of the classifier and the true prior (bottom) Variance of the pseudo-negatives, with threshold in dashed-red, and the minimum number of epochs in dashed-green.}
\centering
\begin{tabular}{@{}cc@{}}
    \includegraphics[width=.5\textwidth]{pics/tweezer_priors.png} &
    \includegraphics[width=.5\textwidth]{pics/brain_priors.png} \\
    \includegraphics[width=.5\textwidth]{pics/slitlamp_priors.png} &
    \includegraphics[width=.5\textwidth]{pics/cochlea_priors.png} \\
\end{tabular}
\label{fig:converge}
\end{figure*}

\subsection{Robustness w.r.t. upper-bound and relevance of frame-wise estimation}
\label{sec:org1ddd2fc}
We wish to assess how our self-supervised prior estimation method performs in a real-world scenario, where the provided upper-bound $\hat \pi_{0}$ is misspecified.
We therefore apply the proposed method \kspss{} using $\hat \pi_{0}=\eta \cdot \max_{i}\pi^{i}$, with $\eta \in \{1.2, 1.4, 1.6, 1.8\}$.
Also, in order to assess the relevance of the self-supervised estimation of priors, we perform segmentation using method \kspconst{} with $\eta \in \{0.8, 1.0, 1.2, 1.4\}$.
As reference, we further segment using the frame-wise true priors, as in method \ksptrue{}.
We report the F1 score, precision and recall in Tab. \ref{tab:results_eta}.

First, we observe that our self-supervised estimation of priors is relatively robust to variations of $\hat \pi_{0}$.
In particular, Tweezer shows variations in F1 scores of max. $1\%$ while $\eta$ varies over range $[1.2, 1.6]$.
Cochlea, varies by $4\%$ over range $[1.2; 1.6]$.
Slitlamp varies by $5\%$ over range $[1.4, 1.8]$.
Last, Brain varies by $4\%$ over range $[1.2, 1.6]$.

These variations are in general more important when using the simpler \kspconst.
In particular, Tweezer varies by $1\%$ over range $[0.8, 1.2]$,
Cochlea varies by $5\%$ over range $[1., 1.4]$,
Slitlamp varies by $15\%$ over range $[0.8, 1.2]$,
and Brain varies by $8\%$ over range $[0.8, 1.2]$.

The relevance of the frame-wise prior estimation is assessed by comparing, for each type, the maximum F1 scores reached by \kspss{} and \kspconst{} for all tested values of $\eta$.
On Tweezer, \kspss{} brings an improvement of $2\%$ over \kspconst{}.
On Cochlea, the improvement is of $6\%$.
On Brain, the improvement is of $1\%$.
Slitlamp, however, shows equal performances for both methods.

Last, comparing \kspss{} to \ksptrue{}, we note that the proposed self-supervised prior estimation framework brings comparable performances on all types.


\input{chapters/nnpu/tables/table_eta.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
