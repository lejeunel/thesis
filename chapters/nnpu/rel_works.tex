\section{Related Works}
\label{sec:rel_works}

%The present paper combines theoretical contributions related to semi-supervised learning, of which Positive/Unlabeled learning is a sub-category, as well as self-%supervised learning.
%From an application point of view, we contribute to the field of sparse point-wise supervision for segmentation.
%We now give an overview of the state-of-the-art related to the present contributions.

In the following section we provide an overview of some of the most relevant related works to the method presented here.

\textbf{Positive/Unlabeled learning} considers the learning setting where only a subset of the positive samples are labeled, while the unlabeled set contains both positive and negative samples.
Early methods focused on iteratively sampling confident negatives from the unlabeled set using a classifier, while re-training the same classifier using these~\cite{li03,liu03,li05}.
In~\cite{lee03}, the authors propose a reweighing scheme applied to the unlabeled samples, which allows the use of traditional supervised classifiers.
As the latter approach heavily relies on appropriate weights,~\cite{elkan08} instead chose to duplicate unlabeled samples into positive and negative samples with complementary weights, an approach called unbiased PU learning. More recently, a general-purpose unbiased risk estimator for PU learning was presented by~\cite{duplessis15} which allows for convex optimization in the PU setting. As a follow-up to the latter,~\cite{kiryo17} noted that modern expressive models, such as Deep Neural Networks, induce negative empirical risks through overfitting of the positives, which they propose to fix by introducing a non-negative unbiased risk estimator. We detail this method in the next section as we build directly from this method.

\textbf{Class-prior estimation} is tightly related to the state-of-the-art PU learning approaches that design risk estimators relying on knowing or estimating the density of positive and negative samples. In~\cite{duplessis14}, the authors suggest to partially match the class-conditional densities of the positive class to the input samples using the Pearson divergence criteria. In~\cite{christoffel16}, the same approach is improved by considering a general divergence criteria along with a $L_{1}$ distance regularization, which diminishes the problem of over-estimation. In~\cite{bekker18}, a tree induction scheme is introduced to estimate the probability of a positive samples as a proxy task~\cite{scott09}. Similar to the self-supervised estimation of class-priors proposed in the present work,~\cite{kato18} combines the non-negative unbiased risk of~\cite{kiryo17} with an iterative update of class-priors. In particular, they devise an update rule inspired by the Expectation-Maximization algorithm and iterate until convergence.

\textbf{Point-wise supervision} was first applied in the context of medical image analysis in~\cite{vilarino2007}, where a Support Vector Machine was used to classify patches. Using a graph approach,~\cite{khosravan16} constructed saliency maps as the input of a Random-Walker to segment CT volumes. More generally,~\cite{bearman16} train a CNN using a loss function that includes an object-prior defined by the user-provided points. The most relevant methods to the present work are those of~\cite{lejeune17} and~\cite{lejeune18}. In the former, a classifier is learned to segment various kinds of medical image volumes and videos in a PU setting using a loss function that leverages the uncertainties of unlabeled samples. As a follow-up,~\cite{lejeune18}, formulated the same problem as a multi-path optimization problem. In particular, a simple object model formulated in a PU setting provides costs of selecting superpixels. The multi-path framework showed good performances in inferring structures that were non-concave (\eg a doughnut shape).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
