\subsection{Training details, hyper-parameters, and implementation}
\label{sec:implementation}
We now specify technical details of our implementation and training procedure. \SSnnPU~is implemented\footnote{Code will be made publicly available.} in Python using the PyTorch library, while we use the publicly available implementation of \KSPTrack\footnote{\url{https://github.com/lejeunel/ksptrack}}.

\subsubsection{SSnnPU}
$f_\theta$ is implemented as a Convolutional Neural Network based on the U-Net architecture proposed in~\cite{ibtehaz20} for all experiments. It uses ``Inception-like'' blocks in place of simple convolutional layers to improve robustness to scale variations. Skip connections are replaced by a serie of $3\times3$ convolutional layers with residual connections. Batch normalization layers are added after each convolutional layer.

To train \SSnnPU, we proceed with a three phase process:
\begin{enumerate}
  \item To increase the robustness of early observations, we train $f$ for 50 epochs with Alg.~\ref{alg:sgdnnpu} and a learning rate set to $10^{-4}$. With the last layer of our decoder being a sigmoid function, we set the bias of the preceding convolutional layer to $-\log{\frac{1-\pi_{init}}{\pi_{init}}}$, with $\pi_{init}=0.01$, as advised in~\cite{lin17}. All others parameters are initialized using He's method~\cite{he15init}.
    \item We then optimize the model and class-prior estimates for a maximum $100$ epochs as described in Alg.~ \ref{alg:prior_estim} with a learning rate set to $10^{-5}$.
  \item We then train using frame-wise priors given by the previous phase for an additional $100$ epochs with a learning rate of $10^{-5}$.
\end{enumerate}
We use the Adam optimizer with weight decay $0.01$ for all training phases. Data augmentation is performed using a random combination of additive gaussian noise, bilateral blur and gamma contrasting. 

\subsubsection{Recursive Bayesian Estimation}
For the process, transition, and initial covariance matrices, we use diagonal matrices 
$Q=\sigma_{Q}\mathbb{I}$, $R=\sigma_{R}\mathbb{I}$, and $S=\sigma_{S}\mathbb{I}$, where $\mathbb{I}$ is the identity matrix.
As the observations $\rho_{k}^{i}$ are often very noisy, we set $\gamma=2$ and the observation variance much larger than the process variance $\sigma_{Q}=10$, $\sigma_{R}=0.05$ and $\sigma_{S}=0.03$.
The parameters of the control input are set proportionally to $\hat \pi_{0}$ with $u_{0}=0.02 \hat \pi_{0}$, and $u_{T}=0.4 \hat \pi_{0}$. The window length of the frame-wise smoothing filter is set proportionally to the number of frames: $L=0.05N$. The time-period of our stopping condition is set to $T_{s}=10$ and the threshold on the variance is $\tau=0.007$.

\subsubsection{KSPTrack parameters}
\label{sec:org4560526}
All sequences are pre-segmented into $\sim 1200$ superpixels and the output of $f$ is averaged over all pixels in a superpixel. Each point-wise annotation defines a circle of radius \(R=0.05 \cdot \max\{W,H\}\) centered on the 2D location, where \(W\) and \(H\) are the width and height of frames, respectively. The input cost at given superpixel is set to $0$ when its centroid is contained within that circle, and $\infty$ otherwise. The transitions costs are set to $0$ when superpixels overlap and $\infty$ otherwise.
In order to reduce the number of edges and alleviate the computational requirements, we also prune {visiting} edges when their corresponding object probability falls below $0.4$. We perform a single round of \KSPTrack~as augmenting the set of positives and re-training the object model after each round (as in~\cite{lejeune18}) did not prove beneficial.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
