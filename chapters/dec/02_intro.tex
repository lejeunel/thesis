\section{Introduction}
\label{sec:intro}

Object segmentation and detection in video and 3D image modalities are crucial problems in medical imaging.
From brain tumors imaged with MRI to surgical instruments in laparoscopic video sequences, supervised machine learning has provided reliable detection and segmention of structures of interest in various medical applications~\cite{menze15,sznitman2014,zikic2014}.
To ensure good prediction performances, a cornerstone of most recent methods and in particular of deep learning methods, is the need for vasts amounts of images and annotations to optimize large numbers of machine learning model parameters. 

Furthermore, while video and 3D image data is increasingly available, generating
ground truth annotations still remains one of the greatest bottlenecks
for creating truly large training datasets. Methods to reduce the annotation
burden have mainly centered on active learning~\cite{KonSznFua15,MosSzn16},
domain adaptation~\cite{ShiRoth16,BerBecSalFua2016} and
crowd-sourcing~\cite{Cheplygina2016}, allowing experts and
non-experts to provide annotations in limited amounts or in parallel. Yet, the
process of providing pixel-wise segmentation ground truth data remains prohibitively slow and tedious regardless. 

In this work, we look to alleviate this burden by proposing a novel method that relies on point-wise, or clicks, annotations of objects of interest.
In this context, a number of methods have been introduced to infer segmentations from sparse 2D annotations in video and 3D volumes.
The work in~\cite{vilarino07} introduced a patch-based SVM within a transductive learning approach to produce segmentations when viewing endoscopy video sequences.
Similarly, \cite{khosravan16} used a gaze tracker to annotate CT volumes with a saliency map-construction and a Random-Walker to segment the object of interest.
An alternative was also presented in~\cite{lejeune17} where an expected exponential loss was used within a Positive-Unknown (PU) learning setting.
In~\cite{bearman16}, a deep learning based approach with point location supervision was used to train a CNN with strong object priors, whereby showing good performances on natural images.
Last, the method introduced in~\cite{lejeune18} relied on multiple instance tracking using limited point-wise supervision showed good generalization on several types of image modalities and objects.
However, this approach suffers several limitations including (i) that the learning of segmentation features are decoupled from the similarity metric used in the tracking optimization and (ii) the use of a superpixel similarity measure is taken as a simple Gaussian distance in the feature space, whereby ignoring potential presence of sub-clusters in the learned latent space.
Both of these negatively impact the performance of the multiple instance tracking and lead to sub-optimal object segmentations. 

To overcome the previously mentioned limitations, we present in this work a novel approach that leverages the assumption that data volumes or sequences are composed of a number of clusters characterezing homogenous regions in the image space. By doing so, our method estimates and refines clusters so to have low intra-variance and high inter-variance in a fully self-supervised fashion~\cite{xie15}. By then simultaneously learning an efficient feature representation and set of clusters (and associations), we use the latter to initialize a Gaussian Mixture Model to model the pairwise similarity of superpixel regions. We show that our approach leads to improved segmentation from point-wise annotations for a wide range of object types across different imaging modalities (video and volumetric) by comparing it to several existing methods. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "00_main"
%%% End:
